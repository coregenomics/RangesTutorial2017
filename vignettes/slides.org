# -*- org-export-babel-evaluate: (quote inline-only);  -*-
#+TITLE: Exploring the Ranges Infrastructure

#+OPTIONS: toc:t H:2
#+PROPERTY: session *R:4*
#+PROPERTY: exports both
#+PROPERTY: results output
#+PROPERTY: eval no-export
#+PROPERTY: tangle yes

#+BEGIN_LaTeX
\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}
#+END_LaTeX

* Introduction
** The Ranges infrastructure: what is it good for?
   [[file:insights.pdf]]

** Integrative data analysis
   [[file:igv.JPG]]
   # Different types of data can be stored in compatible data structures
   # Genomic data are all related through genomic coordinates
   
** Developing and prototyping methods
*** Columns
   :PROPERTIES:
   :BEAMER_env: columns
   :END:
  
**** Peak calling						      :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :BEAMER_env: block
     :END:
    [[file:peak-calling.png]]

**** Isoform expression						      :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :BEAMER_env: block
     :END:
     [[file:isoform.jpg]]
    
**** Variant calling						      :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :BEAMER_env: block
     :END:
     [[file:pileup.png]]
    
** Software integration
   #+ATTR_latex: :width 8cm
   [[file:sticker-integration.pdf]]
   
* Data structures
** Data types
*** Columns
   :PROPERTIES:
   :BEAMER_env: columns
   :END:
**** Data on genomic ranges					      :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :BEAMER_env: block
     :END:
     [[file:karyogram.png]]
    
**** Summarized data 						      :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :BEAMER_env: block
     :END:
     [[file:summarized.jpg]]
  
** Reality
   :PROPERTIES:
   :ID:       B215883B-F6BB-46A6-B7FF-0667AB947F0A
   :END:
   * In practice, we have a BED file:
     #+begin_src bash
     bash-3.2$  ls *.bed
     #+end_src
     #+begin_example
     my.bed
     #+end_example
     
   * And we turn to R to analyze the data
     #+begin_src R
       df <- read.table("my.bed", sep="\t")
       colnames(df) <- c("chrom", "start", "end")
     #+end_src     
     #+begin_example
  chrom     start       end
1  chr7 127471196 127472363
2  chr7 127472363 127473530
3  chr7 127473530 127474697
4  chr9 127474697 127475864
5  chr9 127475864 127477031
     #+end_example
     
** Reality bites
   :PROPERTIES:
   :ID:       2D787635-4F9D-4AF9-B810-AD8BED7B5BB5
   :END:
   Now for a GFF file:
     #+begin_src R
       df <- read.table("my.bed", sep="\t")
       colnames(df) <- c("chr", "start", "end")
     #+end_src
     
*** Columns							  :B_columns:
    :PROPERTIES:
    :BEAMER_env: columns
    :END:
    
*** GFF 							      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_env: block
    :END:

    #+begin_example
   chr     start       end
1 chr7 127471197 127472363
2 chr7 127472364 127473530
3 chr7 127473531 127474697
4 chr9 127474698 127475864
5 chr9 127475865 127477031
    #+end_example
    
*** BED 							      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_env: block
    :END:

    #+begin_example
  chrom     start       end
1  chr7 127471196 127472363
2  chr7 127472363 127473530
3  chr7 127473530 127474697
4  chr9 127474697 127475864
5  chr9 127475864 127477031
     #+end_example

** From reality to ideality
   #+begin_latex
   \framesubtitle{The abstraction gradient}
   #+end_latex
   [[file:evolution.pdf]]
   
   * Abstraction is semantic enrichment
     * Enables the user to think of data in terms of the problem domain
     * Hides implementation details
     * Unifies frameworks

** GRanges: data on genomic ranges
      [[file:granges.pdf]]
   

   | seqnames | start | end | strand | ... |
   |----------+-------+-----+--------+-----|
   | chr1     |     1 |  10 | +      |     |
   | chr1     |    15 |  24 | -      |     |

   * Plus, sequence information (lengths, genome, etc)

** Semantic slack
   [[file:mcols-bridge.pdf]]
   
   * Science defies rigidity: we define flexible objects that combine
     strongly typed fields with arbitrary user-level metadata

** Abstraction is the responsibility of the user
   [[file:data-science-explore-clean.png]]
   
   * Only the user knows the true semantics of the data
   * Explicitly declaring semantics:
     * Helps the software do the right thing
     * Helps the user be more /expressive/

** SummarizedExperiment: the central data model
   [[file:se.jpg]]   

* Algorithms
** The Ranges API
   * Semantically rich data enables:
     * Semantically rich vocabularies and grammars
     * Semantically aware behavior (DWIM)
   * The range algebra expresses typical range-oriented operations
   * Base R API is extended to have range-oriented behaviors

** The Ranges API: Examples
   | Type          | Range operations          | Range extensions     |
   |---------------+---------------------------+----------------------|
   | *Filter*      | subsetByOverlaps()        | [()                  |
   | *Transform*   | shift(), resize()         | *() to zoom          |
   | *Aggregation* | coverage(), reduce()      | intersect(), union() |
   | *Comparison*  | findOverlaps(), nearest() | match(), sort()      |
   
** Range algebra
   [[file:range-algebra.pdf]]

** Overlap detection
    [[file:overlaps.png]]

* Example workflow: Structural variants
** Structural variants are important for disease
   * SVs are rarer than SNVs
     * SNVs: ~ 4,000,000 per genome
     * SVs: 5,000 - 10,000 per genome
   * However, SVs are much larger (typically > 1kb) and cover more
     genomic space than SNVs.
   * The effect size of SV associations with disease is larger than
     those of SNVs.
     * SVs account for 13% of GTEx eQTLs
     * SVs are 26 - 54 X more likely to modulate expression than SNVs
       (or indels)

** Detection of deletions from WGS data
   #+attr_LATEX: :width 11cm
   [[file:sv-detection-1.pdf]]
   
** Motivation
*** Problem
    * Often need to evaluate a tool before adding it to our workflow
    * "lumpy" is a popular SV caller

*** Goal
    Evaluate the performance of lumpy

** Data
   * Simulated a FASTQ containing known deletions using varsim
   * Aligned the reads with BWA
   * Ran lumpy on the alignments

** Overview
   1. Import the lumpy calls and truth set
   2. Tidy the data
   3. Match the calls to the truth
   4. Compute error rates
   5. Diagnose errors
      
** Data import
   Read from VCF:
   #+begin_src R
     library(RangesTutorial2017)
     calls <- readVcf(system.file("extdata", "lumpy.vcf.gz",
                                  package="RangesTutorial2017"))
     truth <- readVcf(system.file("extdata", "truth.vcf.bgz",
                                  package="RangesTutorial2017"))
   #+end_src

   Select for deletions:
   #+begin_src R
   truth <- subset(truth, SVTYPE=="DEL")
   calls <- subset(calls, SVTYPE=="DEL")
   #+end_src
   
** Data cleaning
   Make the seqlevels compatible:
   #+begin_src R
     seqlevelsStyle(calls) <- "NCBI"
     truth <- keepStandardChromosomes(truth,
                                      pruning.mode="coarse")
   #+end_src

** Tighten
   Move from the constrained VCF representation to a range-oriented
   model (/VRanges/) with a tighter cognitive link to the problem:
   #+begin_src R
     calls <- as(calls, "VRanges")
     truth <- as(truth, "VRanges")
   #+end_src

** More cleaning
   Homogenize the ALT field:
   #+begin_src R
   ref(truth) <- "."
   #+end_src
   
   Remove the flagged calls with poor read support:
   #+begin_src R
     calls <- calls[called(calls)]
   #+end_src

** Comparison
   * How to decide whether a call represents a true event?
   * Ranges should at least overlap:
   #+begin_src R
     hits <- findOverlaps(truth, calls)
   #+end_src
   * But more filtering is needed.

** Comparing breakpoints
   Compute the deviation in the breakpoints:
   #+begin_src R
     hits <- as(hits, "List")
     call_rl <- extractList(ranges(calls), hits)
     dev <- abs(start(truth) - start(call_rl)) +
         abs(end(truth) - end(call_rl))
   #+end_src

   Select and store the call with the least deviance, per true deletion:
   #+begin_src R
     dev_ord <- order(dev)
     keep <- phead(dev_ord, 1L)
     truth$deviance <- drop(dev[keep])
     truth$call <- drop(hits[keep])
   #+end_src
   
** Choosing a deviance cutoff
   #+begin_src R
     library(ggplot2)
     rdf <- as.data.frame(truth)
     ggplot(aes(x=deviance),
            data=subset(rdf, deviance <= 500)) +
         stat_ecdf() + ylab("fraction <= deviance")
   #+end_src

** Choosing a deviance cutoff
   #+attr_LATEX: :width 7cm
   [[file:ecdf-deviance.pdf]]

** Applying the deviance filter
   #+begin_src R
     truth$called <-
         with(truth, !is.na(deviance) & deviance <= 300)
   #+end_src
   
** Sensitivity
   #+begin_src R
   mean(truth$called)
   #+end_src

   #+RESULTS:
   : [1] 0.8214107

** Specificity
   Determine which calls were true:
   #+begin_src R
     calls$fp <- TRUE
     calls$fp[subset(truth, called)$call] <- FALSE
   #+end_src

   Compute FDR:
   #+begin_src R
     mean(calls$fp)
   #+end_src

   #+RESULTS:
   : [1] 0.1009852

   
** Explaining the FDR
   :PROPERTIES:
   :ID:       8935398E-2BE9-4C62-B35A-0FFBD1B6F869
   :END:
   * Suspect that calls may be error-prone in regions where the
     population varies
   * Load alt regions from a BED file:
     #+begin_src R
       file <- system.file("extdata",
                           "altRegions.GRCh38.bed.gz",
                           package="RangesTutorial2017")
       altRegions <- import(file)
       seqlevelsStyle(altRegions) <- "NCBI"
       altRegions <-
           keepStandardChromosomes(altRegions,
                                   pruning.mode="coarse")
     #+end_src

** FDR and variable "alt" regions
   * Compute the association between FP status and overlap of an alt
     region:
     #+begin_src R
       calls$inAlt <- calls %over% altRegions
       xtabs(~ inAlt + fp, calls)
     #+end_src

     #+RESULTS:
     :        fp
     : inAlt   FALSE TRUE
     :   FALSE  1402  112
     :   TRUE     58   52

